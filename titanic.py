# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NSLRiha37EqsbodojHiqWPXGxi3mPLSo
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np               # Numerical operations ke liye NumPy use hota hai (arrays, math functions, etc.)
import pandas as pd             # Data handling ke liye Pandas (DataFrames banane aur manipulate karne ke liye)
import matplotlib.pyplot as plt # Data visualization ke liye Matplotlib (graphs, plots banane ke liye)
# %matplotlib inline
import sklearn                  # Machine Learning tools and models (training, testing, metrics waghera ke liye)
import seaborn as sns           # Matplotlib se better and beautiful visualizations ke liye Seaborn
import warnings
warnings.filterwarnings('ignore')  # Console mein warnings ko chupane ke liye (e.g., deprecated warnings)
plt.rcParams["figure.figsize"] = [10,5]  # Yeh galat syntax hai — ismein error hai (comma typo hai)

full_data = pd.read_csv('Titanic-Dataset.csv')

"""**The first step in the machine learning pipeline is to clean and transform the training data into a usable format for analysis and modeling.**

- Data pre-processing addresses:

- Assumptions about data shape

- Incorrect data types

- Outliers or errors

- Missing values

- Categorical variables

**DATA Shape**
"""

print(full_data.shape)

full_data.head()

full_data.info()

#heatmap
sns.heatmap(full_data.isnull(),yticklabels=False,cbar=False,cmap='tab20c_r')
plt.title("Missing Data:Traning set")
plt.show()

"""***The 'Age' variable is missing roughly 20% of its data.
This proportion is likely small enough for reasonable replacements using some form of imputation as well (using the knowledge of the other columns to fill in reasonable values).
However, too much data from the 'Cabin' column is missing to do anything useful with it at a basic level.
This column may need to be dropped from the dataset altogether or changed to another feature such as 'Cabin Known: 1 or 0'.

We want to fill in missing age data instead of just dropping the missing age data rows.
One way to do this is by filling in the mean age of all the passengers (imputation).
However, we can be smarter about this and check the average age by passenger class.***
"""

plt.figure(figsize=(10,5))
sns.boxplot(x='Pclass',y='Age',data=full_data,palette='GnBu_d').set_title('Age Distribution by Passenger Class')
plt.show()

"""**Naturally, the wealthier passengers in the higher classes tend to be older. We'll use these average age values to impute based on Pclass for Age**"""

def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]

    # If Age is missing
    if pd.isnull(Age):
        if Pclass == 1:
            return 37  # Assume age 37 for 1st class passengers
        elif Pclass == 2:
            return 29  # Assume age 29 for 2nd class passengers
        else:
            return 24  # Assume age 24 for 3rd class passengers
    else:
        return Age  # If Age is not missing, return the actual value
full_data['Age'] = full_data[['Age', 'Pclass']].apply(impute_age, axis=1)

"""# The cabin column has to many missing value so it would be best to remove"""

#remove cabin feature
full_data.drop('Cabin',axis=1,inplace=True)

#remove rows with missing data
full_data.dropna(inplace=True)

"""### The columns Name and Ticket can be removed from the dataset because they don’t help in predicting whether a passenger survived or not.

### The remaining columns with text values, like Sex and Embarked, should be turned into categories so we can get better results during analysis.
"""

#Remove unnecessary columns
# full_data.drop(['Name','Ticket'],axis=1,inplace=True)
#conver object to category data type
object_columns = ['Sex','Embarked']
#Convert Strings to Category Type use astype funtion
full_data[object_columns] = full_data[object_columns].astype('category')

full_data

#numeric summary
full_data.describe()

#remove pessengerID
full_data.drop('PassengerId', inplace=True, axis=1)

"""# **Getting Model Ready**

Now that we’ve looked at and understood the data, it’s time to prepare the features for modeling.
We need to convert categorical features (like text columns) into dummy variables (i.e., numbers),
because machine learning models can’t use text directly as input.
"""

#shape of the train data
full_data.shape

#identify the categorial column
full_data.select_dtypes(['category']).columns

full_data.head()

# Convert and assign all at once
full_data = pd.concat([
    full_data,
    pd.get_dummies(full_data['Sex'], drop_first=True).astype(int),
    pd.get_dummies(full_data['Embarked'], drop_first=True).astype(int)
], axis=1)

# Then drop original columns
full_data.drop(['Sex', 'Embarked'], axis=1, inplace=True)

# # Reload original data
# df = pd.read_csv('Titanic-Datasett.csv')

# # Just extract the needed columns
# full_data['Sex'] = df['Sex']
# full_data['Embarked'] = df['Embarked']

full_data

"""Now the training data is ready for machine learning because:

All columns are numeric

Everything is combined into one clean table

## **OBJECTIVE 2: MACHINE LEARNING**

Now, I will use these features with different classification algorithms to see which model performs best.
I will follow a simple process:
**Split, Fit, Predict, Score It**

Now, we will separate the target variable (Survived) from the feature set.
The target variable will be stored in y, and the remaining features will be stored in X.
"""

#SPLIT DATA TO BE USED IN THE MODELS
  # cREATE MATRIX OF FEATURES
x = full_data.drop('Survived',axis=1)
  #CREATE VECTOR OF DEPENDENT VARIABLE
y = full_data['Survived']

"""### Use X and y variables to split the training data into train and test set

"""

# Use X and y variables to split the training data into train and test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.20,random_state=101)

"""###train_test_split() is a function that splits your dataset into a training set and a testing set.

### test_size=0.20 means 20% of the data will be used for testing, and 80% for training.

### random_state=101 ensures reproducibility — you’ll get the same split every time you run it
"""

# Fit
# Import model
from sklearn.linear_model import LogisticRegression

# Create instance of model
lreg = LogisticRegression()

# Pass training data into model
lreg.fit(x_train, y_train)

# predict
y_pred_lreg = lreg.predict(x_test)
print(y_pred_lreg)

#score It
from sklearn.metrics import  classification_report, accuracy_score
print('Classification Model')
#accuracy
print('---------'*30)
logreg_accuracy = accuracy_score(y_test, y_pred_lreg) *100
print('Accuracy', round(logreg_accuracy,2), '%')

